{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install 'apache-beam[gcp]' --quiet --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gooogle-cloud-dataflow --quiet --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import pprint as pp\n",
    "import google.auth.transport.requests\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText, WriteToText\n",
    "from apache_beam.options.pipeline_options import SetupOptions, PipelineOptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting credentials and scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCOPES = ['https://www.googleapis.com/auth/cloud-platform']\n",
    "SERVICE_ACCOUNT_FILE = 'path_of_service_account_json_key'\n",
    "\n",
    "credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "\n",
    "auth_req = google.auth.transport.requests.Request()\n",
    "credentials.refresh(auth_req)\n",
    "access_token = credentials.token\n",
    "\n",
    "headers = {'Authorization': 'Bearer' + access_token,\n",
    "           'Content-Type': 'application/json; charset=utf-8'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create classic template via vertexai jupyter workbench\n",
    "We can directly create a classic template from a vertexai jupyter notebook if is created via the service account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyPipelineOptions(PipelineOptions):\n",
    "    @classmethod\n",
    "    def _add_argparse_args(cls, parser):\n",
    "        # Use add_value_provider_argument for arguments to be templatable\n",
    "        # Use add_argument as usual for non-templatable arguments\n",
    "\n",
    "        parser.add_value_provider_argument('--input',\n",
    "                                           type=str,\n",
    "                                           help='Path of the file to read from')\n",
    "        \n",
    "        parser.add_value_provider_argument('--output',\n",
    "                                           type=str,\n",
    "                                           help='Output file to write results to.')\n",
    "        \n",
    "pipeline_options = {\n",
    "    'project': 'project_id',\n",
    "    'runner': 'DataflowRunner',\n",
    "    'region': 'us-east4',\n",
    "    'staging_location': 'gs://your_staging_bucket_location',\n",
    "    'temp_location': 'gs://your_temp_bucket_location',\n",
    "    'template_location': 'gs://your_classic_template_location'\n",
    "}\n",
    "\n",
    "pipeline_options = PipelineOptions.from_dictionary(pipeline_options)\n",
    "beam_options = pipeline_options.view_as(MyPipelineOptions)\n",
    "\n",
    "pipeline = beam.Pipeline(options=pipeline_options)\n",
    "\n",
    "lines = (pipeline \n",
    "         | 'Read' >> ReadFromText(beam_options.input)\n",
    "         | 'Write' >> WriteToText(beam_options.output, num_shards=1, shard_name_template='' ))\n",
    "\n",
    "pipeline.run().wait_unitl_finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running Dataflow Template via REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = 'project_id'\n",
    "job = f'test-dataflow-job-{datetime.now().strftime(\"%y%m%d%H%M%S\")}' # for unique job name\n",
    "template = 'gs://your_template_location'\n",
    "\n",
    "environment = {\n",
    "    'ipConfiguration': 'WORKER_IP_PRIVATE',\n",
    "    'numWorkers': 4,\n",
    "    'maxWorkers': 8,\n",
    "    'workerRegion': 'us-central1',\n",
    "    'machineType': 'c2-standard-8',\n",
    "    'serviceAccountEmail': 'your_service_account_name',\n",
    "    'subnetwork': 'your_full_subnetwork_url',\n",
    "    'temp_location': 'gs://temp_location_for_beam_temp_file_storage',\n",
    "    'additionalExperiments': ['use_runner_v2']\n",
    "}\n",
    "\n",
    "parameters = {\n",
    "    'input': 'your_runtime_value_provider input_value',\n",
    "    'output': 'your_runtime_value_provider output value'\n",
    "}\n",
    "\n",
    "body = {\n",
    "    'jobName': job,\n",
    "    'parameters': parameters,\n",
    "    'environment': environment\n",
    "}\n",
    "\n",
    "print(\"Request Body for dataflow job.\")\n",
    "pp.pprint(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataflow = build('dataflow',\n",
    "                 'v1b3',\n",
    "                 credentials=credentials)\n",
    "\n",
    "try:\n",
    "    request = dataflow.projects().templates().launch(projectId=project,\n",
    "                                                     gcsPath=template,\n",
    "                                                     body=body)\n",
    "    \n",
    "    response = request.execute()\n",
    "\n",
    "    if(str('error') in response.keys()):\n",
    "        raise NameError(response)\n",
    "    \n",
    "    print('Dataflow Job got triggered : ' + response['job']['name'])\n",
    "\n",
    "except NameError as e:\n",
    "    print(f'Dataflow Pipeline failed : {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate response\n",
    "pp.pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait for all jobs to complete\n",
    "\n",
    "job_execution_completed = False\n",
    "time_var = 0 \n",
    "\n",
    "while not job_execution_completed:\n",
    "    if (time_var >= 900):\n",
    "        dataflow = build('dataflow', 'v1b3')\n",
    "        dataflow.projects()\n",
    "        time_var = 0\n",
    "\n",
    "    job = dataflow.projects().locations().jobs().get(projectId=response['job']['projectId'],\n",
    "                                                     location=response['job']['location'],\n",
    "                                                     jobId=response['job']['jobId']).execute()\n",
    "    \n",
    "    if job['currentState'] != 'JOB_STATE_RUNNING' \\\n",
    "        and job['currentState'] != 'JOB_STATE_PENDING' \\\n",
    "            and job['currentState'] != 'JOB_STATE_QUEUED':\n",
    "        \n",
    "        print('Job id: {} - Job execution status: {} '.format(job['id'], job['currentState']))\n",
    "\n",
    "        if job['currentState'] != 'JOB_STATE_DONE':\n",
    "            raise Exception('Dataflow job got failed. Please check logs for more details.')\n",
    "\n",
    "        job_execution_completed = True\n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "pp.pprint(response) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running Dataflow Template via gcloud command\n",
    "\n",
    "1. To set the active account, run: `gcloud config set account SERVICE_ACCOUNT`\n",
    "\n",
    "2. Validate if the gcloud config is set or not by running: `gcloud config list`\n",
    "\n",
    "Output should look like this -\n",
    "- account = SERVICE_ACCOUNT\n",
    "- disable_usage_reposting = True\n",
    "- project = PROJECT_ID\n",
    "\n",
    "3. If your current active account `SERVICE_ACCOUNT` does not have any valid ccredentials:\n",
    "- Please run: `gcloud auth login` to obtain new cerdentials\n",
    "- For service account, please activate it first: `gcloud auth activate-service-account ACCOUNT`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**POSITIONAL ARGUMENTS**\n",
    "\n",
    "- `JOB_NAME` : The unique name to assign to the job.\n",
    "\n",
    "**REQUIRED FLAGS**\n",
    "\n",
    "`--gcs-location=GCS_LOCATION` : The Google Cloud Storage location of the job template to run. (Must be a URL beginning with 'gs://'.)\n",
    "\n",
    "**OPTIONAL FLAGS**\n",
    "\n",
    "\n",
    "`--additional-experiments=[ADDITIONAL_EXPERIMENTS,]` : Additional experiments to pass to the job. These experiments are appended to any experiments already set by the template.\n",
    "\n",
    "`--disable-public-ips` : The Cloud Dataflow workers must not use public IP addresses. Overrides the default dataflow/disable_public_ips property value for this command invocation.\n",
    "\n",
    "`--max-workers=MAX_WORKERS` : The maximum number of workers to run.\n",
    "\n",
    "`--network=NETWORK` : The Compute Engine network for launching instances to run your pipeline.\n",
    "\n",
    "`--num-workers=NUM_WORKERS` : The initial number of workers to use.\n",
    "\n",
    "`--parameters=[PARAMETERS,â€¦]` : The parameters to pass to the job. Set parameters to the comma-seperated list of parametrs to pass to the job. Spaces between commas and values are not allowed.\n",
    "\n",
    "`--region=REGION_ID` : Region ID of the job's regional endpoint. Defaults to 'us-central1'.\n",
    "\n",
    "`--service-account-email=SERVICE_ACCOUNT_EMAIL` : The service account to run the workers as.\n",
    "\n",
    "`--staging-location=STAGING_LOCATION` : The Google Cloud Storage location to stage temporary files. (Must be a URL beginning with 'gs://'.)\n",
    "\n",
    "`--subnetwork=SUBNETWORK` : The Compute Engine subnetwork for launching instances to run your pipeline.\n",
    "\n",
    "`--worker-machine-type=WORKER_MACHINE_TYPE` : The type of machine to use for workers. Defaults to server-specified.\n",
    "\n",
    "\n",
    "**WORKER LOCATION OPTIONS**\n",
    "\n",
    "At most one of these can be specified:\n",
    "\n",
    "`--worker-region=WORKER_REGION` : The region to run the workers in.\n",
    "\n",
    "`--worker-zone=WORKER_ZONE` : The zone to run the workers in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud dataflow jobs run test-classic-job \\\n",
    "--project projectId \\\n",
    "--gcs-location gs://your_classic_template_bucket_file_location/template.json \\\n",
    "--region us-central1 \\\n",
    "--service-account-email your_service_account_name \\\n",
    "--staging-location gs://your_staging_bucket_location \\\n",
    "--subnetwork your_full_subnetwork_url \\\n",
    "--num-workers 4 \\\n",
    "--max-workers 8 \\\n",
    "--disable-public-ips \\\n",
    "--worker-region us-central1 \\\n",
    "--worker-machine c2-standard-8 \\\n",
    "--parameters input=gs://your_runtime_value_provider_input_value,output=gs://your_runtime_value_provider_output_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
